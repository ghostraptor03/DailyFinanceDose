2025-06-08 15:18:44 [scrapy.utils.log] INFO: Scrapy 2.13.1 started (bot: financial_news_scraper)
2025-06-08 15:18:44 [scrapy.utils.log] INFO: Versions:
{'lxml': '5.4.0',
 'libxml2': '2.11.9',
 'cssselect': '1.3.0',
 'parsel': '1.10.0',
 'w3lib': '2.3.1',
 'Twisted': '21.7.0',
 'Python': '3.13.4 (tags/v3.13.4:8a526ec, Jun  3 2025, 17:46:04) [MSC v.1943 '
           '64 bit (AMD64)]',
 'pyOpenSSL': '25.1.0 (OpenSSL 3.5.0 8 Apr 2025)',
 'cryptography': '45.0.3',
 'Platform': 'Windows-11-10.0.26100-SP0'}
2025-06-08 15:18:44 [scrapy.addons] INFO: Enabled addons:
[]
2025-06-08 15:18:44 [py.warnings] WARNING: C:\Users\zacht\FinanceCrawler\DailyFinanceDose\.venv\Lib\site-packages\scrapy\utils\request.py:120: ScrapyDeprecationWarning: 'REQUEST_FINGERPRINTER_IMPLEMENTATION' is a deprecated setting.
It will be removed in a future version of Scrapy.
  return cls(crawler)

2025-06-08 15:18:44 [scrapy.extensions.telnet] INFO: Telnet Password: 9ee7967b2007b849
2025-06-08 15:18:44 [scrapy.middleware] INFO: Enabled extensions:
['scrapy.extensions.corestats.CoreStats',
 'scrapy.extensions.telnet.TelnetConsole',
 'scrapy.extensions.feedexport.FeedExporter',
 'scrapy.extensions.logstats.LogStats',
 'scrapy.extensions.throttle.AutoThrottle']
2025-06-08 15:18:44 [scrapy.crawler] INFO: Overridden settings:
{'AUTOTHROTTLE_ENABLED': True,
 'AUTOTHROTTLE_MAX_DELAY': 10,
 'AUTOTHROTTLE_START_DELAY': 1,
 'AUTOTHROTTLE_TARGET_CONCURRENCY': 2.0,
 'BOT_NAME': 'financial_news_scraper',
 'CONCURRENT_REQUESTS_PER_DOMAIN': 4,
 'DOWNLOAD_DELAY': 2,
 'LOG_FILE': 'C:\\Users\\zacht\\FinanceCrawler\\DailyFinanceDose\\logs\\scraper_20250608_151844.log',
 'LOG_LEVEL': 'INFO',
 'NEWSPIDER_MODULE': 'financial_news_scraper.spiders',
 'RANDOMIZE_DOWNLOAD_DELAY': 0.5,
 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',
 'SPIDER_MODULES': ['financial_news_scraper.spiders'],
 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
               '(KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36'}
2025-06-08 15:18:44 [scrapy-playwright] INFO: Started loop on separate thread: <ProactorEventLoop running=True closed=False debug=False>
2025-06-08 15:18:44 [scrapy.middleware] INFO: Enabled downloader middlewares:
['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',
 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',
 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',
 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',
 'financial_news_scraper.middlewares.RotateUserAgentMiddleware',
 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',
 'financial_news_scraper.middlewares.ProxyMiddleware',
 'scrapy.downloadermiddlewares.retry.RetryMiddleware',
 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',
 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',
 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',
 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',
 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',
 'scrapy.downloadermiddlewares.stats.DownloaderStats']
2025-06-08 15:18:44 [scrapy.middleware] INFO: Enabled spider middlewares:
['scrapy.spidermiddlewares.start.StartSpiderMiddleware',
 'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',
 'scrapy.spidermiddlewares.referer.RefererMiddleware',
 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',
 'scrapy.spidermiddlewares.depth.DepthMiddleware']
2025-06-08 15:18:44 [scrapy.middleware] INFO: Enabled item pipelines:
['financial_news_scraper.pipelines.ValidationPipeline',
 'financial_news_scraper.pipelines.DuplicationPipeline',
 'financial_news_scraper.pipelines.FinancialFilterPipeline',
 'financial_news_scraper.pipelines.TickerExtractionPipeline',
 'financial_news_scraper.pipelines.SentimentAnalysisPipeline',
 'financial_news_scraper.pipelines.DatabasePipeline',
 'financial_news_scraper.pipelines.ExportPipeline']
2025-06-08 15:19:29 [scrapy.core.engine] INFO: Spider opened
2025-06-08 15:19:29 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2025-06-08 15:19:29 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023
2025-06-08 15:19:29 [scrapy-playwright] INFO: Starting download handler
2025-06-08 15:19:29 [scrapy-playwright] INFO: Starting download handler
2025-06-08 15:19:29 [reuters] INFO: Starting Reuters spider with Playwright (async start).
2025-06-08 15:19:29 [reuters] INFO: Requesting start URL: https://www.reuters.com/business/
2025-06-08 15:19:44 [scrapy.crawler] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2025-06-08 15:19:44 [scrapy.core.engine] INFO: Closing spider (shutdown)
2025-06-08 15:19:44 [scrapy-playwright] INFO: Launching browser chromium
2025-06-08 15:19:44 [scrapy-playwright] INFO: Browser chromium launched
2025-06-08 15:19:58 [scrapy-playwright] WARNING: Closing page due to failed request: <GET https://www.reuters.com/business/> exc_type=<class 'playwright._impl._errors.TimeoutError'> exc_msg=Page.wait_for_selector: Timeout 10000ms exceeded.
Call log:
  - waiting for locator("a[data-testid='Link']") to be visible
Traceback (most recent call last):
  File "C:\Users\zacht\FinanceCrawler\DailyFinanceDose\.venv\Lib\site-packages\scrapy_playwright\handler.py", line 433, in _download_request_with_retry
    return await self._download_request_with_page(request, page, spider)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zacht\FinanceCrawler\DailyFinanceDose\.venv\Lib\site-packages\scrapy_playwright\handler.py", line 481, in _download_request_with_page
    await self._apply_page_methods(page, request, spider)
  File "C:\Users\zacht\FinanceCrawler\DailyFinanceDose\.venv\Lib\site-packages\scrapy_playwright\handler.py", line 632, in _apply_page_methods
    pm.result = await _maybe_await(method(*pm.args, **pm.kwargs))
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zacht\FinanceCrawler\DailyFinanceDose\.venv\Lib\site-packages\scrapy_playwright\_utils.py", line 21, in _maybe_await
    return await obj
           ^^^^^^^^^
  File "C:\Users\zacht\FinanceCrawler\DailyFinanceDose\.venv\Lib\site-packages\playwright\async_api\_generated.py", line 8180, in wait_for_selector
    await self._impl_obj.wait_for_selector(
        selector=selector, timeout=timeout, state=state, strict=strict
    )
  File "C:\Users\zacht\FinanceCrawler\DailyFinanceDose\.venv\Lib\site-packages\playwright\_impl\_page.py", line 425, in wait_for_selector
    return await self._main_frame.wait_for_selector(**locals_to_params(locals()))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\zacht\FinanceCrawler\DailyFinanceDose\.venv\Lib\site-packages\playwright\_impl\_frame.py", line 323, in wait_for_selector
    await self._channel.send("waitForSelector", locals_to_params(locals()))
  File "C:\Users\zacht\FinanceCrawler\DailyFinanceDose\.venv\Lib\site-packages\playwright\_impl\_connection.py", line 61, in send
    return await self._connection.wrap_api_call(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<2 lines>...
    )
    ^
  File "C:\Users\zacht\FinanceCrawler\DailyFinanceDose\.venv\Lib\site-packages\playwright\_impl\_connection.py", line 528, in wrap_api_call
    raise rewrite_error(error, f"{parsed_st['apiName']}: {error}") from None
playwright._impl._errors.TimeoutError: Page.wait_for_selector: Timeout 10000ms exceeded.
Call log:
  - waiting for locator("a[data-testid='Link']") to be visible

2025-06-08 15:19:59 [reuters] ERROR: <twisted.python.failure.Failure playwright._impl._errors.TimeoutError: Page.wait_for_selector: Timeout 10000ms exceeded.
Call log:
  - waiting for locator("a[data-testid='Link']") to be visible
>
2025-06-08 15:19:59 [scrapy.statscollectors] INFO: Dumping Scrapy stats:
{'downloader/exception_count': 1,
 'downloader/exception_type_count/playwright._impl._errors.TimeoutError': 1,
 'downloader/request_bytes': 275,
 'downloader/request_count': 1,
 'downloader/request_method_count/GET': 1,
 'elapsed_time_seconds': 30.43494,
 'finish_reason': 'shutdown',
 'finish_time': datetime.datetime(2025, 6, 8, 20, 19, 59, 740438, tzinfo=datetime.timezone.utc),
 'items_per_minute': 0.0,
 'log_count/ERROR': 2,
 'log_count/INFO': 18,
 'log_count/WARNING': 2,
 'playwright/browser_count': 1,
 'playwright/context_count': 1,
 'playwright/context_count/max_concurrent': 1,
 'playwright/context_count/persistent/False': 1,
 'playwright/context_count/remote/False': 1,
 'playwright/page_count': 1,
 'playwright/page_count/closed': 1,
 'playwright/page_count/max_concurrent': 1,
 'playwright/request_count': 8,
 'playwright/request_count/method/GET': 8,
 'playwright/request_count/navigation': 2,
 'playwright/request_count/resource_type/document': 2,
 'playwright/request_count/resource_type/font': 1,
 'playwright/request_count/resource_type/image': 2,
 'playwright/request_count/resource_type/script': 1,
 'playwright/request_count/resource_type/stylesheet': 2,
 'playwright/response_count': 8,
 'playwright/response_count/method/GET': 8,
 'playwright/response_count/resource_type/document': 2,
 'playwright/response_count/resource_type/font': 1,
 'playwright/response_count/resource_type/image': 2,
 'playwright/response_count/resource_type/script': 1,
 'playwright/response_count/resource_type/stylesheet': 2,
 'responses_per_minute': 0.0,
 'scheduler/dequeued': 1,
 'scheduler/dequeued/memory': 1,
 'scheduler/enqueued': 1,
 'scheduler/enqueued/memory': 1,
 'start_time': datetime.datetime(2025, 6, 8, 20, 19, 29, 305498, tzinfo=datetime.timezone.utc)}
2025-06-08 15:19:59 [scrapy.core.engine] INFO: Spider closed (shutdown)
2025-06-08 15:19:59 [scrapy-playwright] INFO: Closing download handler
2025-06-08 15:20:00 [scrapy-playwright] INFO: Closing download handler
2025-06-08 15:20:00 [scrapy-playwright] INFO: Closing browser
